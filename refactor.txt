
import os
import sys
from concurrent.futures import ThreadPoolExecutor
from pydub import AudioSegment
import librosa
import numpy as np
import soundfile as sf
from scipy.signal import find_peaks
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AudioNormalizer:
    def __init__(self, max_workers=None, verbose=False):
        """Initialize the AudioNormalizer with an optional number of threads."""
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.verbose = verbose

    def normalize_audio(self, input_path, output_path):
        """Normalize audio to -1 dBFS and handle cleanup."""
        try:
            audio = AudioSegment.from_file(input_path)
            normalized_audio = audio.apply_gain(-audio.max_dBFS - 1)  # Normalize without clipping
            normalized_audio.export(output_path, format="wav")
            if self.verbose:
                logging.info(f"Normalized and saved: {output_path}")

            # Perform cleanup: Remove the original file after processing
            os.remove(input_path)
            if self.verbose:
                logging.info(f"Removed original file: {input_path}")
        except Exception as e:
            logging.error(f"Error processing {input_path}: {e}")

    def normalize_in_background(self, input_path, output_path):
        """Run the normalization in a non-blocking way."""
        self.executor.submit(self.normalize_audio, input_path, output_path)


class PianoNoteIdentifier:
    STANDARD_A4 = 440.0  # Frequency of A4 (440 Hz)
    CENTS_FLAT = 60  # 60 cents flat tuning

    MIDI_NOTES = {  # MIDI note numbers mapped to note names
        21: "A0", 22: "A#0", 23: "B0", 24: "C1", 25: "C#1", 26: "D1", 27: "D#1", 28: "E1", 29: "F1", 30: "F#1", 31: "G1", 32: "G#1",
        33: "A1", 34: "A#1", 35: "B1", 36: "C2", 37: "C#2", 38: "D2", 39: "D#2", 40: "E2", 41: "F2", 42: "F#2", 43: "G2", 44: "G#2",
        45: "A2", 46: "A#2", 47: "B2", 48: "C3", 49: "C#3", 50: "D3", 51: "D#3", 52: "E3", 53: "F3", 54: "F#3", 55: "G3", 56: "G#3",
        57: "A3", 58: "A#3", 59: "B3", 60: "C4", 61: "C#4", 62: "D4", 63: "D#4", 64: "E4", 65: "F4", 66: "F#4", 67: "G4", 68: "G#4",
        69: "A4", 70: "A#4", 71: "B4", 72: "C5", 73: "C#5", 74: "D5", 75: "D#5", 76: "E5", 77: "F5", 78: "F#5", 79: "G5", 80: "G#5",
        81: "A5", 82: "A#5", 83: "B5", 84: "C6", 85: "C#6", 86: "D6", 87: "D#6", 88: "E6", 89: "F6", 90: "F#6", 91: "G6", 92: "G#6",
        93: "A6", 94: "A#6", 95: "B6", 96: "C7", 97: "C#7", 98: "D7", 99: "D#7", 100: "E7", 101: "F7", 102: "F#7", 103: "G7", 104: "G#7",
        105: "A7", 106: "A#7", 107: "B7", 108: "C8"
    }

    @staticmethod
    def frequency_to_midi(frequency):
        return 69 + 12 * np.log2(frequency / PianoNoteIdentifier.STANDARD_A4)

    @staticmethod
    def get_note_name(frequency):
        adjusted_frequency = frequency * 2 ** (-PianoNoteIdentifier.CENTS_FLAT / 1200)
        midi_note = round(PianoNoteIdentifier.frequency_to_midi(adjusted_frequency))
        return PianoNoteIdentifier.MIDI_NOTES.get(midi_note, None)

    @staticmethod
    def detect_pitch(audio, sr):
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
        max_index = np.unravel_index(np.argmax(magnitudes), magnitudes.shape)
        pitch = pitches[max_index]
        return pitch if pitch > 0 else None

    def identify_note_and_name(self, audio, sr):
        pitch = self.detect_pitch(audio, sr)
        return self.get_note_name(pitch) if pitch else "hammer"


class TransientDetector:
    def __init__(self, hop_length=256, verbose=False):
        self.hop_length = hop_length
        self.verbose = verbose

    def detect_transients(self, audio, sr, threshold=0.05, min_distance=1.0):
        onset_env = librosa.onset.onset_strength(y=audio, sr=sr, hop_length=self.hop_length)
        peaks, _ = find_peaks(
            onset_env,
            height=threshold * np.max(onset_env),
            distance=int(min_distance * sr / self.hop_length)
        )
        return librosa.frames_to_time(peaks, sr=sr, hop_length=self.hop_length)

    def process_transients(self, audio, sr, transients, output_dir, normalizer):
        note_identifier = PianoNoteIdentifier()
        for i, start_time in enumerate(transients):
            end_time = transients[i + 1] if i + 1 < len(transients) else len(audio) / sr
            clip_path = os.path.join(output_dir, f"note_{i + 1}.wav")
            AudioManager.save_audio_clip(audio, sr, start_time, end_time, clip_path)
            note_name = note_identifier.identify_note_and_name(audio[int(start_time * sr):int(end_time * sr)], sr)
            if note_name != "hammer":
                renamed_path = os.path.join(output_dir, f"{note_name}_note_{i + 1}.wav")
                os.rename(clip_path, renamed_path)
                clip_path = renamed_path
            normalizer.normalize_in_background(clip_path, os.path.join(output_dir, f"normalized_{os.path.basename(clip_path)}"))


class AudioManager:
    @staticmethod
    def load_audio(audio_path):
        try:
            return librosa.load(audio_path, sr=None)
        except Exception as e:
            logging.error(f"Error loading {audio_path}: {e}")
            return None, None

    @staticmethod
    def save_audio_clip(audio, sr, start_time, end_time, output_path):
        start_sample = int(start_time * sr)
        end_sample = int(min(end_time * sr, len(audio)))
        if start_sample < end_sample:
            sf.write(output_path, audio[start_sample:end_sample], sr)


class AudioProcessor:
    def __init__(self, normalizer, transient_detector, verbose=False):
        self.normalizer = normalizer
        self.transient_detector = transient_detector
        self.verbose = verbose

    def process_audio_file(self, audio_path, output_dir):
        audio, sr = AudioManager.load_audio(audio_path)
        if audio is None:
            return
        transients = self.transient_detector.detect_transients(audio, sr)
        self.transient_detector.process_transients(audio, sr, transients, output_dir, self.normalizer)

    def process_audio_files(self, input_files, output_dir):
        for audio_file in input_files:
            if self.verbose:
                logging.info(f"Processing {audio_file}...")
            self.process_audio_file(audio_file, output_dir)


if __name__ == "__main__":
    if len(sys.argv) < 3:
        logging.error("Usage: python script.py <input_audio_files> <output_directory>")
        sys.exit(1)

    input_files = sys.argv[1:-1]
    output_directory = sys.argv[-1]
    os.makedirs(output_directory, exist_ok=True)

    normalizer = AudioNormalizer(max_workers=4, verbose=True)
    transient_detector = TransientDetector(hop_length=256, verbose=True)
    processor = AudioProcessor(normalizer, transient_detector, verbose=True)

    processor.process_audio_files(input_files, output_directory)
